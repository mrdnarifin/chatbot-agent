{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9685142-45d9-45f1-87c2-69316792b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a 4-line poem about the ocean.\n",
      "---------- assistant ----------\n",
      "Beneath the waves, a world so vast,  \n",
      "Whispers of secrets from ages past.  \n",
      "Blue depths embrace the sun's golden gleam,  \n",
      "In the ocean's heart, we dream and dream.  \n",
      "TERMINATE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response:  lagi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user_proxy ----------\n",
      "lagi\n",
      "---------- assistant ----------\n",
      "Waves dance and shimmer under the moon,  \n",
      "A lullaby of tides, a timeless tune.  \n",
      "Endless horizons where sky meets sea,  \n",
      "The ocean's embrace, wild and free.  \n",
      "TERMINATE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response:  approve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user_proxy ----------\n",
      "approve\n",
      "---------- assistant ----------\n",
      "TERMINATE\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your response:  APPROVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user_proxy ----------\n",
      "APPROVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a 4-line poem about the ocean.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=46, completion_tokens=48), content=\"Beneath the waves, a world so vast,  \\nWhispers of secrets from ages past.  \\nBlue depths embrace the sun's golden gleam,  \\nIn the ocean's heart, we dream and dream.  \\nTERMINATE\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='42fac688-18b9-435e-99c8-ebcf52aa6798', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='lagi', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=106, completion_tokens=44), content=\"Waves dance and shimmer under the moon,  \\nA lullaby of tides, a timeless tune.  \\nEndless horizons where sky meets sea,  \\nThe ocean's embrace, wild and free.  \\nTERMINATE\", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='8543eff3-e1a7-4f20-9e82-bec4666965d8', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='approve', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=162, completion_tokens=4), content='TERMINATE', type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, request_id='36dd2410-0c6a-4b02-b5ae-31f75bb5ad0c', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, content='APPROVE', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", input_func=input)  # Use input() to get user input from console.\n",
    "\n",
    "# Create the termination condition which will end the conversation when the user says \"APPROVE\".\n",
    "termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# Create the team.\n",
    "team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)\n",
    "\n",
    "# Run the conversation and stream to the console.\n",
    "stream = team.run_stream(task=\"Write a 4-line poem about the ocean.\")\n",
    "# Use asyncio.run(...) when running in a script.\n",
    "await Console(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c332d2b-41d2-4d8c-88e7-37a85a99fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a 4-line poem about the ocean.\n",
      "---------- assistant ----------\n",
      "In azure depths where secrets lie,  \n",
      "Waves whisper tales as seagulls fly.  \n",
      "A dance of tides, both wild and free,  \n",
      "The ocean's heart, a mystery.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your feedback (type 'exit' to leave):  exit\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create the agents.\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "assistant = AssistantAgent(\"assistant\", model_client=model_client)\n",
    "\n",
    "# Create the team setting a maximum number of turns to 1.\n",
    "team = RoundRobinGroupChat([assistant], max_turns=1)\n",
    "\n",
    "task = \"Write a 4-line poem about the ocean.\"\n",
    "while True:\n",
    "    # Run the conversation and stream to the console.\n",
    "    stream = team.run_stream(task=task)\n",
    "    # Use asyncio.run(...) when running in a script.\n",
    "    await Console(stream)\n",
    "    # Get the user response.\n",
    "    task = input(\"Enter your feedback (type 'exit' to leave): \")\n",
    "    if task.lower().strip() == \"exit\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4736f65f-edc8-41e3-a732-1589eea36488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "What is the weather in New York?\n",
      "---------- lazy_assistant ----------\n",
      "[FunctionCall(id='call_X4EhmmDlLtRGEqn0mMRHUyAe', arguments='{}', name='transfer_to_user')]\n",
      "[Prompt tokens: 69, Completion tokens: 12]\n",
      "---------- lazy_assistant ----------\n",
      "[FunctionExecutionResult(content='Transfer to user.', call_id='call_X4EhmmDlLtRGEqn0mMRHUyAe')]\n",
      "---------- lazy_assistant ----------\n",
      "Transfer to user.\n",
      "---------- Summary ----------\n",
      "Number of messages: 4\n",
      "Finish reason: Handoff to user from lazy_assistant detected.\n",
      "Total prompt tokens: 69\n",
      "Total completion tokens: 12\n",
      "Duration: 1.52 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), ToolCallRequestEvent(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=69, completion_tokens=12), content=[FunctionCall(id='call_X4EhmmDlLtRGEqn0mMRHUyAe', arguments='{}', name='transfer_to_user')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='lazy_assistant', models_usage=None, content=[FunctionExecutionResult(content='Transfer to user.', call_id='call_X4EhmmDlLtRGEqn0mMRHUyAe')], type='ToolCallExecutionEvent'), HandoffMessage(source='lazy_assistant', models_usage=None, target='user', content='Transfer to user.', context=[], type='HandoffMessage')], stop_reason='Handoff to user from lazy_assistant detected.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.base import Handoff\n",
    "from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    ")\n",
    "\n",
    "# Create a lazy assistant agent that always hands off to the user.\n",
    "lazy_agent = AssistantAgent(\n",
    "    \"lazy_assistant\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n",
    "    system_message=\"If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that checks for handoff messages.\n",
    "handoff_termination = HandoffTermination(target=\"user\")\n",
    "# Define a termination condition that checks for a specific text mention.\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "\n",
    "# Create a single-agent team with the lazy assistant and both termination conditions.\n",
    "lazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)\n",
    "\n",
    "# Run the team and stream to the console.\n",
    "task = \"What is the weather in New York?\"\n",
    "await Console(lazy_agent_team.run_stream(task=task), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78e17e0-5e1d-47e9-a298-d147ae7ccdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    # api_key=\"sk-...\", # Optional if you have an OPENAI_API_KEY env variable set.\n",
    ")\n",
    "\n",
    "# Create the primary agent.\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    ")\n",
    "\n",
    "# Create the critic agent.\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064fad24-366f-4fd4-876e-06651cff7d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a unique, Haiku about the weather in Paris\n",
      "---------- primary ----------\n",
      "Cobblestones glisten,  \n",
      "Gentle rain whispers secrets—  \n",
      "Paris breathes in mist.  \n",
      "[Prompt tokens: 76, Completion tokens: 22]\n",
      "---------- critic ----------\n",
      "Your haiku effectively paints a delicate picture of Paris under the cloak of a gentle rain. The imagery of \"cobblestones glisten\" and \"rain whispers secrets\" is evocative and adds a layer of intimacy to the poem. The line \"Paris breathes in mist\" beautifully captures the essence of the city’s ambiance. To enhance the haiku further, you might want to incorporate a detail that indicates either the time of day or the season, which could provide additional depth and context to the scene. Overall, it's a well-crafted and atmospheric haiku.\n",
      "\n",
      "APPROVE\n",
      "[Prompt tokens: 231, Completion tokens: 118]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: Maximum number of messages 3 reached, current message count: 3\n",
      "Total prompt tokens: 307\n",
      "Total completion tokens: 140\n",
      "Duration: 5.75 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=76, completion_tokens=22), content='Cobblestones glisten,  \\nGentle rain whispers secrets—  \\nParis breathes in mist.  ', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=231, completion_tokens=118), content='Your haiku effectively paints a delicate picture of Paris under the cloak of a gentle rain. The imagery of \"cobblestones glisten\" and \"rain whispers secrets\" is evocative and adds a layer of intimacy to the poem. The line \"Paris breathes in mist\" beautifully captures the essence of the city’s ambiance. To enhance the haiku further, you might want to incorporate a detail that indicates either the time of day or the season, which could provide additional depth and context to the scene. Overall, it\\'s a well-crafted and atmospheric haiku.\\n\\nAPPROVE', type='TextMessage')], stop_reason='Maximum number of messages 3 reached, current message count: 3')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_msg_termination = MaxMessageTermination(max_messages=3)\n",
    "round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)\n",
    "\n",
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48dba57d-68c9-472f-ba7e-22e4a250bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- critic ----------\n",
      "I'm glad you found the feedback helpful and that you're satisfied with the revised haiku. It's a beautiful piece that captures the essence of Paris in an evocative way. If you have any more writing you'd like feedback on or any other topics you'd like to explore, don't hesitate to reach out. Great work and happy writing!\n",
      "\n",
      "APPROVE\n",
      "[Prompt tokens: 545, Completion tokens: 68]\n",
      "---------- primary ----------\n",
      "Thank you so much for your supportive feedback! I'm thrilled that you enjoyed it. If there's anything else you'd like to write about or if you have any questions, feel free to ask. Happy writing to you as well!\n",
      "[Prompt tokens: 490, Completion tokens: 45]\n",
      "---------- critic ----------\n",
      "Thank you for your kind words! I'm here to help with any writing projects or questions you might have. Feel free to reach out whenever you need assistance or feedback. Wishing you continued creativity and enjoyment in your writing journey! Happy writing! \n",
      "\n",
      "APPROVE\n",
      "[Prompt tokens: 668, Completion tokens: 53]\n",
      "---------- Summary ----------\n",
      "Number of messages: 3\n",
      "Finish reason: Maximum number of messages 3 reached, current message count: 3\n",
      "Total prompt tokens: 1703\n",
      "Total completion tokens: 166\n",
      "Duration: 8.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=545, completion_tokens=68), content=\"I'm glad you found the feedback helpful and that you're satisfied with the revised haiku. It's a beautiful piece that captures the essence of Paris in an evocative way. If you have any more writing you'd like feedback on or any other topics you'd like to explore, don't hesitate to reach out. Great work and happy writing!\\n\\nAPPROVE\", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=490, completion_tokens=45), content=\"Thank you so much for your supportive feedback! I'm thrilled that you enjoyed it. If there's anything else you'd like to write about or if you have any questions, feel free to ask. Happy writing to you as well!\", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=668, completion_tokens=53), content=\"Thank you for your kind words! I'm here to help with any writing projects or questions you might have. Feel free to reach out whenever you need assistance or feedback. Wishing you continued creativity and enjoyment in your writing journey! Happy writing! \\n\\nAPPROVE\", type='TextMessage')], stop_reason='Maximum number of messages 3 reached, current message count: 3')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "611c038a-7240-4382-8dc7-9482a6bcd67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Write a unique, Haiku about the weather in Paris\n",
      "---------- primary ----------\n",
      "Chill winds softly dance,  \n",
      "Leaves swirl by the Seine's gray flow—  \n",
      "Paris dons autumn.  \n",
      "---------- critic ----------\n",
      "Your haiku beautifully evokes the feel of autumn in Paris. The phrase \"chill winds softly dance\" captures the seasonal change with elegance, and \"leaves swirl by the Seine's gray flow\" provides a vivid image that ties the natural elements to the city’s iconic river. The closing line, \"Paris dons autumn,\" effectively conveys the transformation of the city as it embraces the season. To further enhance the sensory experience, you might consider incorporating another sensory detail, such as a sound or smell, to deepen the scene. Overall, it's a wonderfully atmospheric haiku.\n",
      "\n",
      "APPROVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=556, completion_tokens=23), content=\"Chill winds softly dance,  \\nLeaves swirl by the Seine's gray flow—  \\nParis dons autumn.  \", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=770, completion_tokens=118), content='Your haiku beautifully evokes the feel of autumn in Paris. The phrase \"chill winds softly dance\" captures the seasonal change with elegance, and \"leaves swirl by the Seine\\'s gray flow\" provides a vivid image that ties the natural elements to the city’s iconic river. The closing line, \"Paris dons autumn,\" effectively conveys the transformation of the city as it embraces the season. To further enhance the sensory experience, you might consider incorporating another sensory detail, such as a sound or smell, to deepen the scene. Overall, it\\'s a wonderfully atmospheric haiku.\\n\\nAPPROVE', type='TextMessage')], stop_reason=\"Text 'APPROVE' mentioned\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_msg_termination = MaxMessageTermination(max_messages=10)\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "combined_termination = max_msg_termination | text_termination\n",
    "\n",
    "round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=combined_termination)\n",
    "\n",
    "# Use asyncio.run(...) if you are running this script as a standalone script.\n",
    "await Console(round_robin_team.run_stream(task=\"Write a unique, Haiku about the weather in Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34c49347-6567-48c0-867b-fab8d2bf6512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10...\n",
      "9...\n",
      "8...\n",
      "7...\n",
      "6...\n",
      "5...\n",
      "4...\n",
      "3...\n",
      "2...\n",
      "1...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from typing import AsyncGenerator, List, Sequence, Tuple\n",
    "\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.base import Response\n",
    "from autogen_agentchat.messages import AgentEvent, ChatMessage, TextMessage\n",
    "from autogen_core import CancellationToken\n",
    "\n",
    "\n",
    "class CountDownAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str, count: int = 10):\n",
    "        super().__init__(name, \"A simple agent that counts down.\")\n",
    "        self._count = count\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        # Calls the on_messages_stream.\n",
    "        response: Response | None = None\n",
    "        async for message in self.on_messages_stream(messages, cancellation_token):\n",
    "            if isinstance(message, Response):\n",
    "                response = message\n",
    "        assert response is not None\n",
    "        return response\n",
    "\n",
    "    async def on_messages_stream(\n",
    "        self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken\n",
    "    ) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:\n",
    "        inner_messages: List[AgentEvent | ChatMessage] = []\n",
    "        for i in range(self._count, 0, -1):\n",
    "            msg = TextMessage(content=f\"{i}...\", source=self.name)\n",
    "            inner_messages.append(msg)\n",
    "            yield msg\n",
    "        # The response is returned at the end of the stream.\n",
    "        # It contains the final message and all the inner messages.\n",
    "        yield Response(chat_message=TextMessage(content=\"Done!\", source=self.name), inner_messages=inner_messages)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "async def run_countdown_agent() -> None:\n",
    "    # Create a countdown agent.\n",
    "    countdown_agent = CountDownAgent(\"countdown\")\n",
    "\n",
    "    # Run the agent with a given task and stream the response.\n",
    "    async for message in countdown_agent.on_messages_stream([], CancellationToken()):\n",
    "        if isinstance(message, Response):\n",
    "            print(message.chat_message.content)\n",
    "        else:\n",
    "            print(message.content)\n",
    "\n",
    "\n",
    "# Use asyncio.run(run_countdown_agent()) when running in a script.\n",
    "await run_countdown_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7df334-95fd-4ac1-bd51-8603157bb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Sequence\n",
    "\n",
    "from autogen_agentchat.agents import BaseChatAgent\n",
    "from autogen_agentchat.base import Response\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.messages import ChatMessage\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "class ArithmeticAgent(BaseChatAgent):\n",
    "    def __init__(self, name: str, description: str, operator_func: Callable[[int], int]) -> None:\n",
    "        super().__init__(name, description=description)\n",
    "        self._operator_func = operator_func\n",
    "        self._message_history: List[ChatMessage] = []\n",
    "\n",
    "    @property\n",
    "    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n",
    "        return (TextMessage,)\n",
    "\n",
    "    async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:\n",
    "        # Update the message history.\n",
    "        # NOTE: it is possible the messages is an empty list, which means the agent was selected previously.\n",
    "        self._message_history.extend(messages)\n",
    "        # Parse the number in the last message.\n",
    "        assert isinstance(self._message_history[-1], TextMessage)\n",
    "        number = int(self._message_history[-1].content)\n",
    "        # Apply the operator function to the number.\n",
    "        result = self._operator_func(number)\n",
    "        # Create a new message with the result.\n",
    "        response_message = TextMessage(content=str(result), source=self.name)\n",
    "        # Update the message history.\n",
    "        self._message_history.append(response_message)\n",
    "        # Return the response.\n",
    "        return Response(chat_message=response_message)\n",
    "\n",
    "    async def on_reset(self, cancellation_token: CancellationToken) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d60ed4a-7408-4fb8-847d-0b0869b68735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- user ----------\n",
      "Apply the operations to turn the given number into 25.\n",
      "---------- user ----------\n",
      "10\n",
      "---------- multiply_agent ----------\n",
      "20\n",
      "---------- add_agent ----------\n",
      "21\n",
      "---------- multiply_agent ----------\n",
      "42\n",
      "---------- subtract_agent ----------\n",
      "41\n",
      "---------- subtract_agent ----------\n",
      "40\n",
      "---------- subtract_agent ----------\n",
      "39\n",
      "---------- subtract_agent ----------\n",
      "38\n",
      "---------- subtract_agent ----------\n",
      "37\n",
      "---------- Summary ----------\n",
      "Number of messages: 10\n",
      "Finish reason: Maximum number of messages 10 reached, current message count: 10\n",
      "Total prompt tokens: 0\n",
      "Total completion tokens: 0\n",
      "Duration: 6.80 seconds\n"
     ]
    }
   ],
   "source": [
    "async def run_number_agents() -> None:\n",
    "    # Create agents for number operations.\n",
    "    add_agent = ArithmeticAgent(\"add_agent\", \"Adds 1 to the number.\", lambda x: x + 1)\n",
    "    multiply_agent = ArithmeticAgent(\"multiply_agent\", \"Multiplies the number by 2.\", lambda x: x * 2)\n",
    "    subtract_agent = ArithmeticAgent(\"subtract_agent\", \"Subtracts 1 from the number.\", lambda x: x - 1)\n",
    "    divide_agent = ArithmeticAgent(\"divide_agent\", \"Divides the number by 2 and rounds down.\", lambda x: x // 2)\n",
    "    identity_agent = ArithmeticAgent(\"identity_agent\", \"Returns the number as is.\", lambda x: x)\n",
    "\n",
    "    # The termination condition is to stop after 10 messages.\n",
    "    termination_condition = MaxMessageTermination(10)\n",
    "\n",
    "    # Create a selector group chat.\n",
    "    selector_group_chat = SelectorGroupChat(\n",
    "        [add_agent, multiply_agent, subtract_agent, divide_agent, identity_agent],\n",
    "        model_client=OpenAIChatCompletionClient(model=\"gpt-4o\"),\n",
    "        termination_condition=termination_condition,\n",
    "        allow_repeated_speaker=True,  # Allow the same agent to speak multiple times, necessary for this task.\n",
    "        selector_prompt=(\n",
    "            \"Available roles:\\n{roles}\\nTheir job descriptions:\\n{participants}\\n\"\n",
    "            \"Current conversation history:\\n{history}\\n\"\n",
    "            \"Please select the most appropriate role for the next message, and only return the role name.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Run the selector group chat with a given task and stream the response.\n",
    "    task: List[ChatMessage] = [\n",
    "        TextMessage(content=\"Apply the operations to turn the given number into 25.\", source=\"user\"),\n",
    "        TextMessage(content=\"10\", source=\"user\"),\n",
    "    ]\n",
    "    stream = selector_group_chat.run_stream(task=task)\n",
    "    await Console(stream, output_stats=True)\n",
    "\n",
    "\n",
    "# Use asyncio.run(run_number_agents()) when running in a script.\n",
    "await run_number_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2378792a-742e-45b4-b694-bfe9794bf38b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
